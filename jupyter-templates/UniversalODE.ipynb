{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33aefc8f",
   "metadata": {},
   "source": [
    "# Universal ODEs with Catalax\n",
    "\n",
    "This template demonstrates how to use Universal Ordinary Differential Equations (Universal ODEs) with the Catalax JAX library to model systems where part of the dynamics are known and part are unknown. Universal ODEs combine mechanistic knowledge with neural networks to capture both understood physics and unknown behaviors in a single, unified model.\n",
    "\n",
    "## What are Universal ODEs?\n",
    "\n",
    "Universal ODEs are a hybrid modeling approach that combines traditional differential equations (representing known physics) with neural networks (representing unknown dynamics). This allows you to leverage existing mechanistic knowledge while letting the neural network learn the parts of the system that are not well understood or too complex to model explicitly.\n",
    "\n",
    "The key advantages of Universal ODEs include:\n",
    "\n",
    "- **Physics-Informed Learning**: Incorporate existing mechanistic knowledge while learning unknown dynamics\n",
    "- **Interpretable Models**: Separate known physics from learned components for better understanding\n",
    "- **Data Efficiency**: Leverage prior knowledge to reduce data requirements\n",
    "- **Robust Extrapolation**: Physics constraints help the model generalize beyond training data\n",
    "- **Residual Modeling**: Learn corrections to existing models or capture missing physics\n",
    "\n",
    "## How Universal ODEs Work\n",
    "\n",
    "In a Universal ODE, the system dynamics are split into two components:\n",
    "\n",
    "1. **Known Physics**: Traditional differential equations representing well-understood mechanisms\n",
    "2. **Unknown Dynamics**: Neural networks that learn residual or missing behaviors from data\n",
    "\n",
    "The total dynamics become: \n",
    "\n",
    "$$\\frac{dx}{dt} = f_{physics}(x, \\theta) + f_{neural}(x, \\phi)$$\n",
    "\n",
    "Where $f_{physics}$ represents the known mechanistic model and $f_{neural}$ is a neural network that captures unknown or residual dynamics.\n",
    "\n",
    "## Applications in Enzyme Kinetics\n",
    "\n",
    "Universal ODEs are particularly powerful for enzyme kinetics where:\n",
    "\n",
    "- Basic kinetic mechanisms are known but regulatory effects are complex\n",
    "- Allosteric or cooperative effects are present but difficult to model explicitly\n",
    "- Environmental factors influence kinetics in unknown ways\n",
    "- Multiple enzymes interact in ways that are hard to capture mechanistically\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This template provides the basic framework for building Universal ODE models with Catalax. The neural network component will learn to capture any dynamics not explained by your mechanistic model, making it perfect for discovering new biological phenomena or improving model accuracy.\n",
    "\n",
    "Learn more about Universal ODEs with Catalax in the [Catalax documentation](https://catalax.mintlify.app/neural/universal-ode). Also checkout [this example](https://github.com/JR-1991/Catalax/blob/master/examples/UniversalODE.ipynb) for an advanced dowstream application to recover mathematical expressions from the learned dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "%pip install pyenzyme catalax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de39726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.nn as jnn\n",
    "import pyenzyme as pe\n",
    "import catalax as ctx\n",
    "import catalax.neural as ctn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9f95a",
   "metadata": {},
   "source": [
    "In the following cell, we will load the EnzymML document from the EnzymeML Suite. The resulting object is an instance of the `EnzymeMLDocument` class, which you can inspect and re-use for your analysis. The following functions are available and compatible with the EnzymeMLDocument class:\n",
    "\n",
    "- `pe.summary(enzmldoc)`: Print a summary of the EnzymeML document.\n",
    "- `pe.plot(enzmldoc)`: Plot the EnzymeML document.\n",
    "- `pe.plot_interactive(enzmldoc)`: Interactive plot of the EnzymeML document.\n",
    "- `pe.to_pandas(enzmldoc)`: Convert the EnzymeML document to a pandas DataFrame.\n",
    "- `pe.to_sbml(enzmldoc)`: Convert the EnzymeML document to an SBML document.\n",
    "- `pe.to_petab(enzmldoc)`: Convert the EnzymeML document to a PEtab format.\n",
    "- `pe.get_current()`: Get the current EnzymeML document from the EnzymeML Suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9307ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the EnzymeML Suite\n",
    "suite = pe.EnzymeMLSuite()\n",
    "\n",
    "# Get the current EnzymeML document\n",
    "enzmldoc = suite.get_current()\n",
    "\n",
    "# Print a summary of the EnzymeML document\n",
    "pe.summary(enzmldoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdf68a",
   "metadata": {},
   "source": [
    "## Converting EnzymeML to Catalax\n",
    "\n",
    "The `ctx.from_enzymeml` function converts an EnzymeML document to a Catalax dataset and model objects. The dataset contains the experimental data, and the model is a Catalax model object that you can use for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, model = ctx.from_enzymeml(enzmldoc)\n",
    "\n",
    "# We will augment the dataset to generate more data for training\n",
    "# and improve the generalization of the model\n",
    "train_dataset = dataset.augment(n_augmentations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89b1f1",
   "metadata": {},
   "source": [
    "## Step 1: Define the Neural ODE\n",
    "\n",
    "As a first step, we will define the Universal ODE model. This is done by calling the `ctn.UniversalODE.from_model` function, which will determine the states that are modeled within the Universal ODE. This will include your mechanistic model into the neural model.\n",
    "\n",
    "Ther neural part of the Universal ODE is a neural network, and thus requires certain hyperparameters defining its architecture:\n",
    "\n",
    "- `width_size`: The number of neurons in the hidden layer. Keep it small for universal ODEs, it should not dominate the dynamics of the system.\n",
    "- `depth`: The number of hidden layers. Keep it small for universal ODEs, it should not dominate the dynamics of the system.\n",
    "- `activation`: The activation function of the neural network.\n",
    "\n",
    "In the following cell, we will define a Neural ODE model with 4 neurons in the hidden layer and 2 hidden layers, and a `celu` activation function. CELU in particular is a good choice for the activation function, as it is a smooth and non-linear function, which is well-suited for the integration of the ODE. The choice of the activation function is critical and determines the behavior of the Neural ODE. Try to **not** use typical activation functions like `relu`, as they are not well-suited for the integration of the ODE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe31a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a universal ODE model\n",
    "universal_ode = ctn.UniversalODE.from_model(\n",
    "    model,\n",
    "    width_size=4,\n",
    "    depth=2,\n",
    "    activation=jnn.celu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1136a8",
   "metadata": {},
   "source": [
    "## Step 2: Set up a training strategy\n",
    "\n",
    "Now that we have defined the Neural ODE model, we can set up a training strategy. This is done by calling the `ctn.Strategy` class, which will determine the training strategy for the Neural ODE. This strategy will determine the optimizer, the loss function, the batch size, and the learning rate of the training process.\n",
    "\n",
    "Neural networks are typically trained using gradient descent, which explores the parameter space gradually. Hence, our strategy determines how we move through this parameter space. The learning rate `lr` determines how big of a step we take in the parameter space. Too big and we might overshoot the minimum, too small and we will take forever to converge. Hence, the strategy helps us to determine when to take bigger or smaller steps.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "- Initial training steps should only be trained on the first 15% of the data. Think of it as a warmup phase, because otherwise convergence is unstable.\n",
    "- The learning rate should be set to a high value, as we want to explore the parameter space as fast as possible.\n",
    "- After the warmup phase, the learning rate should be decreased to a lower value, as we want to fine-tune the parameters.\n",
    "- The batch size should be set to a value that is a good compromise between memory efficiency and convergence speed.\n",
    "\n",
    "_Note: The defaults have been set to a good compromise for this example. You can try to change the parameters and see how the training behaves._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf928cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use L2 regularization to prevent overfitting\n",
    "penalties = ctn.Penalties.for_universal_ode()\n",
    "\n",
    "# We will use a batch size of 20 and a learning rate of 1e-3\n",
    "strategy = ctn.Strategy(penalties=penalties, batch_size=20)\n",
    "\n",
    "# We will first train on the first 15% of the data with a learning rate of 1e-3\n",
    "strategy.add_step(lr=1e-3, length=0.15, steps=1000)\n",
    "\n",
    "# Then we will train on the rest of the data with a learning rate of 1e-3\n",
    "strategy.add_step(lr=1e-3, steps=1000)\n",
    "\n",
    "# Finally, we will fine-tune the parameters with a learning rate of 1e-4\n",
    "strategy.add_step(lr=1e-4, steps=1000)\n",
    "\n",
    "# Train universal ODE\n",
    "trained = universal_ode.train(\n",
    "    dataset=train_dataset,\n",
    "    strategy=strategy,\n",
    "    print_every=10,\n",
    "    weight_scale=1e-6,\n",
    "    save_milestones=False,  # Set to True to save model checkpoints\n",
    "    # log=\"progress.log\", # Uncomment this line to log progress\n",
    ")\n",
    "\n",
    "# Save the trained model to a file\n",
    "trained.save_to_eqx(\".\", \"trained_universal_ode.eqx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0153e64",
   "metadata": {},
   "source": [
    "_Important: We have not used a validation strategy, as we are only interested in the training process. However, in practice, you should always use a validation strategy to monitor the performance of the model on unseen data. Check out the [Catalax documentation](https://catalax.mintlify.app/basic/data-management#data-splitting-and-cross-validation) for more information._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9af17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained model fit to the data\n",
    "dataset.plot(\n",
    "    predictor=trained,\n",
    "    show=True,\n",
    "    path=\"trained_universal_ode.png\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catalax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
