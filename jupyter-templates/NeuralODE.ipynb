{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33aefc8f",
   "metadata": {},
   "source": [
    "# Neural ODEs with Catalax\n",
    "\n",
    "This template demonstrates how to use Neural Ordinary Differential Equations (Neural ODEs) with the Catalax JAX library to model enzyme kinetic systems. Neural ODEs combine the power of neural networks with differential equations, enabling you to learn complex dynamics directly from data while maintaining the interpretability and physical constraints of ODE-based models.\n",
    "\n",
    "## What are Neural ODEs?\n",
    "\n",
    "Neural ODEs are a class of deep learning models that use neural networks to parameterize the derivative function in an ordinary differential equation. Instead of discretizing time and using fixed layers like traditional neural networks, Neural ODEs treat the hidden state as a continuous function of time, solving an ODE to compute the output.\n",
    "\n",
    "The key advantages of Neural ODEs include:\n",
    "\n",
    "- **Continuous Dynamics**: Model systems with continuous-time dynamics rather than discrete steps\n",
    "- **Memory Efficiency**: Constant memory cost regardless of evaluation time points\n",
    "- **Adaptive Computation**: Automatically adjust computational effort based on dynamics complexity\n",
    "- **Physical Interpretability**: Maintain ODE structure while learning complex behaviors\n",
    "- **Irregular Time Series**: Handle data with non-uniform time sampling naturally\n",
    "\n",
    "## How Neural ODEs Work\n",
    "\n",
    "In a Neural ODE, the dynamics are defined as:\n",
    "\n",
    "$$\\frac{dx}{dt} = f_Î¸(x(t), t)$$\n",
    "\n",
    "Where $f_\\theta$ is a neural network with parameters $\\theta$ that learns the derivative function. The solution $x(t)$ is obtained by solving this ODE using numerical integration methods, with the neural network gradients computed using the adjoint sensitivity method for efficient backpropagation.\n",
    "\n",
    "## Applications in Enzyme Kinetics\n",
    "\n",
    "Neural ODEs are particularly powerful for enzyme kinetics where:\n",
    "\n",
    "- Traditional kinetic models are too simple to capture observed behavior\n",
    "- Complex regulatory mechanisms are present but not well understood\n",
    "- Multiple interacting pathways create non-linear dynamics\n",
    "- Data-driven discovery of new kinetic mechanisms is desired\n",
    "- High-dimensional systems with many species and reactions\n",
    "\n",
    "## Surrogate Modeling\n",
    "\n",
    "Neural ODEs can serve as fast surrogate models for complex mechanistic models, enabling:\n",
    "\n",
    "- **Accelerated Simulation**: Orders of magnitude faster evaluation than traditional ODE solvers\n",
    "- **Bayesian Inference**: Fast surrogates enable practical MCMC sampling for parameter estimation\n",
    "- **Real-time Applications**: Enable real-time model evaluation and control\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This template provides the basic framework for building Neural ODE models with Catalax. The neural network will learn to capture the underlying dynamics of your enzyme kinetic system directly from time-series data, providing both accurate predictions and insights into the system behavior.\n",
    "\n",
    "Learn more about Neural ODEs with Catalax in the [Catalax documentation](https://catalax.mintlify.app/neural/neural-ode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "%pip install pyenzyme catalax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de39726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.nn as jnn\n",
    "import pyenzyme as pe\n",
    "import catalax as ctx\n",
    "import catalax.neural as ctn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9f95a",
   "metadata": {},
   "source": [
    "In the following cell, we will load the EnzymML document from the EnzymeML Suite. The resulting object is an instance of the `EnzymeMLDocument` class, which you can inspect and re-use for your analysis. The following functions are available and compatible with the EnzymeMLDocument class:\n",
    "\n",
    "- `pe.summary(enzmldoc)`: Print a summary of the EnzymeML document.\n",
    "- `pe.plot(enzmldoc)`: Plot the EnzymeML document.\n",
    "- `pe.plot_interactive(enzmldoc)`: Interactive plot of the EnzymeML document.\n",
    "- `pe.to_pandas(enzmldoc)`: Convert the EnzymeML document to a pandas DataFrame.\n",
    "- `pe.to_sbml(enzmldoc)`: Convert the EnzymeML document to an SBML document.\n",
    "- `pe.to_petab(enzmldoc)`: Convert the EnzymeML document to a PEtab format.\n",
    "- `pe.get_current()`: Get the current EnzymeML document from the EnzymeML Suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9307ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the EnzymeML Suite\n",
    "suite = pe.EnzymeMLSuite()\n",
    "\n",
    "# Get the current EnzymeML document\n",
    "enzmldoc = suite.get_current()\n",
    "\n",
    "# Print a summary of the EnzymeML document\n",
    "pe.summary(enzmldoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdf68a",
   "metadata": {},
   "source": [
    "## Converting EnzymeML to Catalax\n",
    "\n",
    "The `ctx.from_enzymeml` function converts an EnzymeML document to a Catalax dataset and model objects. The dataset contains the experimental data, and the model is a Catalax model object that you can use for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, model = ctx.from_enzymeml(enzmldoc)\n",
    "\n",
    "# We will augment the dataset to generate more data for training\n",
    "# and improve the generalization of the model\n",
    "train_dataset = dataset.augment(n_augmentations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89b1f1",
   "metadata": {},
   "source": [
    "## Step 1: Define the Neural ODE\n",
    "\n",
    "As a first step, we will define the Neural ODE model. This is done by calling the `ctn.NeuralODE.from_model` function, which will determine the states that are modeled within the Neural ODE. Please note, you need to make sure that your measurement data aligns with the defined states. Otherwise, the model will not be able to learn the dynamics.\n",
    "\n",
    "Ther inner core of a Neural ODE is a neural network, and thus requires certain hyperparameters defining its architecture:\n",
    "\n",
    "- `width_size`: The number of neurons in the hidden layer.\n",
    "- `depth`: The number of hidden layers.\n",
    "- `activation`: The activation function of the neural network.\n",
    "\n",
    "In the following cell, we will define a Neural ODE model with 4 neurons in the hidden layer and 2 hidden layers, and a `celu` activation function. CELU in particular is a good choice for the activation function, as it is a smooth and non-linear function, which is well-suited for the integration of the ODE. The choice of the activation function is critical and determines the behavior of the Neural ODE. Try to **not** use typical activation functions like `relu`, as they are not well-suited for the integration of the ODE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe31a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural ODE model\n",
    "neural_ode = ctn.NeuralODE.from_model(\n",
    "    model,\n",
    "    width_size=4,\n",
    "    depth=2,\n",
    "    activation=jnn.celu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1136a8",
   "metadata": {},
   "source": [
    "## Step 2: Set up a training strategy\n",
    "\n",
    "Now that we have defined the Neural ODE model, we can set up a training strategy. This is done by calling the `ctn.Strategy` class, which will determine the training strategy for the Neural ODE. This strategy will determine the optimizer, the loss function, the batch size, and the learning rate of the training process.\n",
    "\n",
    "Neural networks are typically trained using gradient descent, which explores the parameter space gradually. Hence, our strategy determines how we move through this parameter space. The learning rate `lr` determines how big of a step we take in the parameter space. Too big and we might overshoot the minimum, too small and we will take forever to converge. Hence, the strategy helps us to determine when to take bigger or smaller steps.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "- Initial training steps should only be trained on the first 15% of the data. Think of it as a warmup phase, because otherwise convergence is unstable.\n",
    "- The learning rate should be set to a high value, as we want to explore the parameter space as fast as possible.\n",
    "- After the warmup phase, the learning rate should be decreased to a lower value, as we want to fine-tune the parameters.\n",
    "- The batch size should be set to a value that is a good compromise between memory efficiency and convergence speed.\n",
    "\n",
    "_Note: The defaults have been set to a good compromise for this example. You can try to change the parameters and see how the training behaves._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf928cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use L2 regularization to prevent overfitting\n",
    "penalties = ctn.Penalties.for_neural_ode(l2_alpha=1e-5)\n",
    "\n",
    "# We will use a batch size of 20 and a learning rate of 1e-3\n",
    "strategy = ctn.Strategy(penalties=penalties, batch_size=20)\n",
    "\n",
    "# We will first train on the first 15% of the data with a learning rate of 1e-3\n",
    "strategy.add_step(lr=1e-3, length=0.15, steps=1000)\n",
    "\n",
    "# Then we will train on the rest of the data with a learning rate of 1e-3\n",
    "strategy.add_step(lr=1e-3, steps=1000)\n",
    "\n",
    "# Finally, we will fine-tune the parameters with a learning rate of 1e-4\n",
    "strategy.add_step(lr=1e-4, steps=1000)\n",
    "\n",
    "# Train neural ODE\n",
    "trained = neural_ode.train(\n",
    "    dataset=train_dataset,\n",
    "    strategy=strategy,\n",
    "    print_every=10,\n",
    "    weight_scale=1e-6,\n",
    "    save_milestones=False,  # Set to True to save model checkpoints\n",
    "    # log=\"progress.log\", # Uncomment this line to log progress\n",
    ")\n",
    "\n",
    "# Save the trained model to a file\n",
    "trained.save_to_eqx(\".\", \"trained_neural_ode.eqx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0153e64",
   "metadata": {},
   "source": [
    "_Important: We have not used a validation strategy, as we are only interested in the training process. However, in practice, you should always use a validation strategy to monitor the performance of the model on unseen data. Check out the [Catalax documentation](https://catalax.mintlify.app/basic/data-management#data-splitting-and-cross-validation) for more information._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9af17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained model fit to the data\n",
    "dataset.plot(\n",
    "    predictor=trained,\n",
    "    show=True,\n",
    "    path=\"trained_neural_ode.png\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catalax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
